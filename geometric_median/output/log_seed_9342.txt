save path : ./output
{'arch': 'resnet20', 'batch_size': 128, 'data_path': './input/cifar.python', 'dataset': 'cifar10', 'decay': 0.0005, 'dist_type': 'l2', 'epoch_prune': 1, 'epochs': 10, 'evaluate': False, 'gammas': [0.2, 0.2, 0.2], 'layer_begin': 0, 'layer_end': 54, 'layer_inter': 3, 'learning_rate': 0.1, 'manualSeed': 9342, 'momentum': 0.9, 'ngpu': 1, 'pretrain_path': '', 'print_freq': 200, 'rate_dist': 0.1, 'rate_norm': 1.0, 'resume': '', 'save_path': './output', 'schedule': [60, 120, 160], 'start_epoch': 0, 'use_cuda': True, 'use_pretrain': False, 'use_state_dict': False, 'workers': 2}
Random Seed: 9342
python version : 3.6.10 | packaged by conda-forge | (default, Apr 24 2020, 16:44:11)  [GCC 7.3.0]
torch  version : 1.4.0
cudnn  version : 7603
Norm Pruning Rate: 1.0
Distance Pruning Rate: 0.1
Layer Begin: 0
Layer End: 54
Layer Inter: 3
Epoch prune: 1
use pretrain: False
Pretrain path: 
Dist type: l2
=> creating model 'resnet20'
=> network :
 CifarResNet(
  (conv_1_3x3): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (stage_1): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_2): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (stage_3): Sequential(
    (0): ResNetBasicblock(
      (conv_a): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): DownsampleA(
        (avg): AvgPool2d(kernel_size=1, stride=2, padding=0)
      )
    )
    (1): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ResNetBasicblock(
      (conv_a): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_a): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv_b): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn_b): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AvgPool2d(kernel_size=8, stride=8, padding=0)
  (classifier): Linear(in_features=64, out_features=10, bias=True)
)
=> do not use any checkpoint for resnet20 model
  **Test** Prec@1 10.000 Prec@5 51.480 Error@1 90.000
  **Test** Prec@1 10.000 Prec@5 46.740 Error@1 90.000

==>>[2021-04-17 06:49:27] [Epoch=000/010] [Need: 00:00:00] [learning_rate=0.1000] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/391]   Time 0.265 (0.265)   Data 0.161 (0.161)   Loss 3.7153 (3.7153)   Prec@1 6.250 (6.250)   Prec@5 53.125 (53.125)   [2021-04-17 06:49:27]
  Epoch: [000][200/391]   Time 0.023 (0.028)   Data 0.000 (0.008)   Loss 1.2448 (1.6754)   Prec@1 58.594 (38.759)   Prec@5 94.531 (87.531)   [2021-04-17 06:49:33]
  **Train** Prec@1 47.106 Prec@5 91.212 Error@1 52.894
  **Test** Prec@1 50.940 Prec@5 94.180 Error@1 49.060
  **Test** Prec@1 50.940 Prec@5 94.180 Error@1 49.060

==>>[2021-04-17 06:49:41] [Epoch=001/010] [Need: 00:02:06] [learning_rate=0.1000] [Best : Accuracy=50.94, Error=49.06]
  Epoch: [001][000/391]   Time 0.210 (0.210)   Data 0.191 (0.191)   Loss 1.0921 (1.0921)   Prec@1 56.250 (56.250)   Prec@5 96.875 (96.875)   [2021-04-17 06:49:42]
  Epoch: [001][200/391]   Time 0.030 (0.027)   Data 0.000 (0.007)   Loss 0.9549 (1.0228)   Prec@1 64.844 (63.390)   Prec@5 98.438 (96.650)   [2021-04-17 06:49:47]
  **Train** Prec@1 65.570 Prec@5 97.014 Error@1 34.430
  **Test** Prec@1 61.710 Prec@5 96.090 Error@1 38.290
  **Test** Prec@1 61.710 Prec@5 96.090 Error@1 38.290

==>>[2021-04-17 06:49:55] [Epoch=002/010] [Need: 00:01:51] [learning_rate=0.1000] [Best : Accuracy=61.71, Error=38.29]
  Epoch: [002][000/391]   Time 0.176 (0.176)   Data 0.153 (0.153)   Loss 0.9191 (0.9191)   Prec@1 69.531 (69.531)   Prec@5 96.875 (96.875)   [2021-04-17 06:49:55]
  Epoch: [002][200/391]   Time 0.014 (0.027)   Data 0.000 (0.008)   Loss 0.7423 (0.8223)   Prec@1 71.094 (70.857)   Prec@5 97.656 (97.777)   [2021-04-17 06:50:01]
  **Train** Prec@1 72.170 Prec@5 98.022 Error@1 27.830
  **Test** Prec@1 67.720 Prec@5 98.250 Error@1 32.280
  **Test** Prec@1 67.720 Prec@5 98.250 Error@1 32.280

==>>[2021-04-17 06:50:09] [Epoch=003/010] [Need: 00:01:38] [learning_rate=0.1000] [Best : Accuracy=67.72, Error=32.28]
  Epoch: [003][000/391]   Time 0.185 (0.185)   Data 0.164 (0.164)   Loss 0.5904 (0.5904)   Prec@1 80.469 (80.469)   Prec@5 99.219 (99.219)   [2021-04-17 06:50:10]
  Epoch: [003][200/391]   Time 0.014 (0.028)   Data 0.000 (0.008)   Loss 0.5883 (0.7167)   Prec@1 78.906 (75.097)   Prec@5 96.094 (98.383)   [2021-04-17 06:50:15]
  **Train** Prec@1 75.528 Prec@5 98.418 Error@1 24.472
  **Test** Prec@1 72.220 Prec@5 97.970 Error@1 27.780
  **Test** Prec@1 72.220 Prec@5 97.970 Error@1 27.780

==>>[2021-04-17 06:50:23] [Epoch=004/010] [Need: 00:01:23] [learning_rate=0.1000] [Best : Accuracy=72.22, Error=27.78]
  Epoch: [004][000/391]   Time 0.187 (0.187)   Data 0.158 (0.158)   Loss 0.6085 (0.6085)   Prec@1 79.688 (79.688)   Prec@5 100.000 (100.000)   [2021-04-17 06:50:23]
  Epoch: [004][200/391]   Time 0.036 (0.029)   Data 0.000 (0.006)   Loss 0.6157 (0.6640)   Prec@1 75.000 (77.309)   Prec@5 99.219 (98.527)   [2021-04-17 06:50:29]
  **Train** Prec@1 77.402 Prec@5 98.548 Error@1 22.598
  **Test** Prec@1 68.150 Prec@5 97.810 Error@1 31.850
  **Test** Prec@1 68.150 Prec@5 97.810 Error@1 31.850

==>>[2021-04-17 06:50:38] [Epoch=005/010] [Need: 00:01:10] [learning_rate=0.1000] [Best : Accuracy=72.22, Error=27.78]
  Epoch: [005][000/391]   Time 0.189 (0.189)   Data 0.164 (0.164)   Loss 0.6556 (0.6556)   Prec@1 75.000 (75.000)   Prec@5 98.438 (98.438)   [2021-04-17 06:50:38]
  Epoch: [005][200/391]   Time 0.026 (0.029)   Data 0.000 (0.006)   Loss 0.4986 (0.6287)   Prec@1 81.250 (78.413)   Prec@5 99.219 (98.686)   [2021-04-17 06:50:44]
  **Train** Prec@1 78.646 Prec@5 98.710 Error@1 21.354
  **Test** Prec@1 70.190 Prec@5 97.810 Error@1 29.810
  **Test** Prec@1 70.190 Prec@5 97.810 Error@1 29.810

==>>[2021-04-17 06:50:52] [Epoch=006/010] [Need: 00:00:56] [learning_rate=0.1000] [Best : Accuracy=72.22, Error=27.78]
  Epoch: [006][000/391]   Time 0.207 (0.207)   Data 0.178 (0.178)   Loss 0.6094 (0.6094)   Prec@1 76.562 (76.562)   Prec@5 100.000 (100.000)   [2021-04-17 06:50:53]
  Epoch: [006][200/391]   Time 0.029 (0.027)   Data 0.000 (0.006)   Loss 0.6397 (0.5899)   Prec@1 77.344 (79.516)   Prec@5 99.219 (98.850)   [2021-04-17 06:50:58]
  **Train** Prec@1 79.426 Prec@5 98.832 Error@1 20.574
  **Test** Prec@1 76.490 Prec@5 98.400 Error@1 23.510
  **Test** Prec@1 76.490 Prec@5 98.400 Error@1 23.510

==>>[2021-04-17 06:51:06] [Epoch=007/010] [Need: 00:00:42] [learning_rate=0.1000] [Best : Accuracy=76.49, Error=23.51]
  Epoch: [007][000/391]   Time 0.185 (0.185)   Data 0.166 (0.166)   Loss 0.7071 (0.7071)   Prec@1 74.219 (74.219)   Prec@5 100.000 (100.000)   [2021-04-17 06:51:06]
  Epoch: [007][200/391]   Time 0.018 (0.027)   Data 0.000 (0.006)   Loss 0.5773 (0.5729)   Prec@1 81.250 (79.998)   Prec@5 100.000 (98.884)   [2021-04-17 06:51:12]
  **Train** Prec@1 79.930 Prec@5 98.898 Error@1 20.070
  **Test** Prec@1 74.340 Prec@5 98.630 Error@1 25.660
  **Test** Prec@1 74.340 Prec@5 98.630 Error@1 25.660

==>>[2021-04-17 06:51:20] [Epoch=008/010] [Need: 00:00:28] [learning_rate=0.1000] [Best : Accuracy=76.49, Error=23.51]
  Epoch: [008][000/391]   Time 0.190 (0.190)   Data 0.163 (0.163)   Loss 0.5978 (0.5978)   Prec@1 82.031 (82.031)   Prec@5 97.656 (97.656)   [2021-04-17 06:51:20]
  Epoch: [008][200/391]   Time 0.026 (0.028)   Data 0.000 (0.007)   Loss 0.4952 (0.5590)   Prec@1 82.812 (80.512)   Prec@5 100.000 (99.024)   [2021-04-17 06:51:25]
  **Train** Prec@1 80.502 Prec@5 98.962 Error@1 19.498
  **Test** Prec@1 77.120 Prec@5 98.620 Error@1 22.880
  **Test** Prec@1 77.120 Prec@5 98.620 Error@1 22.880

==>>[2021-04-17 06:51:33] [Epoch=009/010] [Need: 00:00:14] [learning_rate=0.1000] [Best : Accuracy=77.12, Error=22.88]
  Epoch: [009][000/391]   Time 0.185 (0.185)   Data 0.158 (0.158)   Loss 0.5716 (0.5716)   Prec@1 80.469 (80.469)   Prec@5 99.219 (99.219)   [2021-04-17 06:51:34]
  Epoch: [009][200/391]   Time 0.039 (0.027)   Data 0.000 (0.006)   Loss 0.6075 (0.5367)   Prec@1 79.688 (81.289)   Prec@5 98.438 (99.063)   [2021-04-17 06:51:39]
  **Train** Prec@1 81.018 Prec@5 98.966 Error@1 18.982
  **Test** Prec@1 74.700 Prec@5 97.850 Error@1 25.300
  **Test** Prec@1 74.700 Prec@5 97.850 Error@1 25.300
