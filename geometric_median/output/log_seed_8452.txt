save path : ./output
{'arch': 'resnet20', 'batch_size': 128, 'data_path': './input/cifar.python', 'dataset': 'cifar10', 'decay': 0.0005, 'dist_type': 'l2', 'epoch_prune': 1, 'epochs': 10, 'evaluate': False, 'gammas': [0.2, 0.2, 0.2], 'layer_begin': 0, 'layer_end': 54, 'layer_inter': 3, 'learning_rate': 0.1, 'manualSeed': 8452, 'momentum': 0.9, 'ngpu': 1, 'pretrain_path': '', 'print_freq': 200, 'rate_dist': 0.1, 'rate_norm': 1.0, 'resume': '', 'save_path': './output', 'schedule': [60, 120, 160], 'start_epoch': 0, 'use_cuda': True, 'use_pretrain': False, 'use_state_dict': False, 'workers': 2}
Random Seed: 8452
python version : 3.6.10 | packaged by conda-forge | (default, Apr 24 2020, 16:44:11)  [GCC 7.3.0]
torch  version : 1.4.0
cudnn  version : 7603
Norm Pruning Rate: 1.0
Distance Pruning Rate: 0.1
Layer Begin: 0
Layer End: 54
Layer Inter: 3
Epoch prune: 1
use pretrain: False
Pretrain path: 
Dist type: l2
=> creating model 'resnet20'
=> network :
 MobileNetV2(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (layers): Sequential(
    (0): Block(
      (conv1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Block(
      (conv1): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=96, bias=False)
      (bn2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(16, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (2): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (3): Block(
      (conv1): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)
      (bn2): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (4): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (5): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (6): Block(
      (conv1): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)
      (bn2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (7): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (8): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (9): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (10): Block(
      (conv1): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)
      (bn2): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (11): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (12): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (13): Block(
      (conv1): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)
      (bn2): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (14): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (15): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential()
    )
    (16): Block(
      (conv1): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv2): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
      (bn2): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (conv3): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (shortcut): Sequential(
        (0): Conv2d(160, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
  )
  (conv2): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
  (bn2): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear): Linear(in_features=1280, out_features=10, bias=True)
)
=> do not use any checkpoint for resnet20 model
  **Test** Prec@1 94.100 Prec@5 99.820 Error@1 5.900
  **Test** Prec@1 93.300 Prec@5 99.750 Error@1 6.700

==>>[2021-04-17 07:10:28] [Epoch=000/010] [Need: 00:00:00] [learning_rate=0.1000] [Best : Accuracy=0.00, Error=100.00]
  Epoch: [000][000/391]   Time 0.620 (0.620)   Data 0.172 (0.172)   Loss 0.0018 (0.0018)   Prec@1 100.000 (100.000)   Prec@5 100.000 (100.000)   [2021-04-17 07:10:29]
  Epoch: [000][200/391]   Time 0.045 (0.050)   Data 0.000 (0.001)   Loss 1.4178 (1.6333)   Prec@1 48.438 (40.011)   Prec@5 96.875 (87.601)   [2021-04-17 07:10:38]
  **Train** Prec@1 46.732 Prec@5 90.788 Error@1 53.268
  **Test** Prec@1 40.170 Prec@5 88.580 Error@1 59.830
  **Test** Prec@1 40.170 Prec@5 88.580 Error@1 59.830

==>>[2021-04-17 07:10:52] [Epoch=001/010] [Need: 00:03:30] [learning_rate=0.1000] [Best : Accuracy=40.17, Error=59.83]
  Epoch: [001][000/391]   Time 0.252 (0.252)   Data 0.186 (0.186)   Loss 1.2023 (1.2023)   Prec@1 55.469 (55.469)   Prec@5 94.531 (94.531)   [2021-04-17 07:10:52]
  Epoch: [001][200/391]   Time 0.047 (0.049)   Data 0.000 (0.001)   Loss 0.9444 (1.1298)   Prec@1 61.719 (59.289)   Prec@5 98.438 (95.612)   [2021-04-17 07:11:01]
  **Train** Prec@1 60.930 Prec@5 96.010 Error@1 39.070
  **Test** Prec@1 57.130 Prec@5 96.150 Error@1 42.870
  **Test** Prec@1 57.130 Prec@5 96.150 Error@1 42.870

==>>[2021-04-17 07:11:15] [Epoch=002/010] [Need: 00:03:06] [learning_rate=0.1000] [Best : Accuracy=57.13, Error=42.87]
  Epoch: [002][000/391]   Time 0.225 (0.225)   Data 0.176 (0.176)   Loss 1.1223 (1.1223)   Prec@1 59.375 (59.375)   Prec@5 98.438 (98.438)   [2021-04-17 07:11:15]
  Epoch: [002][200/391]   Time 0.047 (0.048)   Data 0.000 (0.001)   Loss 0.7856 (0.9669)   Prec@1 68.750 (65.423)   Prec@5 98.438 (97.151)   [2021-04-17 07:11:24]
  **Train** Prec@1 66.138 Prec@5 97.234 Error@1 33.862
  **Test** Prec@1 60.730 Prec@5 96.100 Error@1 39.270
  **Test** Prec@1 60.730 Prec@5 96.100 Error@1 39.270

==>>[2021-04-17 07:11:37] [Epoch=003/010] [Need: 00:02:41] [learning_rate=0.1000] [Best : Accuracy=60.73, Error=39.27]
  Epoch: [003][000/391]   Time 0.234 (0.234)   Data 0.187 (0.187)   Loss 0.8183 (0.8183)   Prec@1 67.188 (67.188)   Prec@5 97.656 (97.656)   [2021-04-17 07:11:38]
  Epoch: [003][200/391]   Time 0.046 (0.048)   Data 0.000 (0.001)   Loss 0.8763 (0.8740)   Prec@1 66.406 (69.240)   Prec@5 96.094 (97.641)   [2021-04-17 07:11:47]
  **Train** Prec@1 70.234 Prec@5 97.734 Error@1 29.766
  **Test** Prec@1 63.110 Prec@5 96.260 Error@1 36.890
  **Test** Prec@1 63.110 Prec@5 96.260 Error@1 36.890

==>>[2021-04-17 07:12:00] [Epoch=004/010] [Need: 00:02:17] [learning_rate=0.1000] [Best : Accuracy=63.11, Error=36.89]
  Epoch: [004][000/391]   Time 0.214 (0.214)   Data 0.164 (0.164)   Loss 0.8770 (0.8770)   Prec@1 72.656 (72.656)   Prec@5 97.656 (97.656)   [2021-04-17 07:12:00]
  Epoch: [004][200/391]   Time 0.045 (0.048)   Data 0.000 (0.001)   Loss 0.6510 (0.7986)   Prec@1 76.562 (72.291)   Prec@5 99.219 (97.889)   [2021-04-17 07:12:09]
  **Train** Prec@1 72.686 Prec@5 97.980 Error@1 27.314
  **Test** Prec@1 68.600 Prec@5 97.550 Error@1 31.400
  **Test** Prec@1 68.600 Prec@5 97.550 Error@1 31.400

==>>[2021-04-17 07:12:23] [Epoch=005/010] [Need: 00:01:54] [learning_rate=0.1000] [Best : Accuracy=68.60, Error=31.40]
  Epoch: [005][000/391]   Time 0.264 (0.264)   Data 0.200 (0.200)   Loss 0.7438 (0.7438)   Prec@1 75.000 (75.000)   Prec@5 99.219 (99.219)   [2021-04-17 07:12:23]
  Epoch: [005][200/391]   Time 0.047 (0.048)   Data 0.000 (0.001)   Loss 0.7902 (0.7469)   Prec@1 76.562 (74.075)   Prec@5 96.094 (98.266)   [2021-04-17 07:12:32]
  **Train** Prec@1 74.102 Prec@5 98.248 Error@1 25.898
  **Test** Prec@1 70.630 Prec@5 97.930 Error@1 29.370
  **Test** Prec@1 70.630 Prec@5 97.930 Error@1 29.370

==>>[2021-04-17 07:12:45] [Epoch=006/010] [Need: 00:01:31] [learning_rate=0.1000] [Best : Accuracy=70.63, Error=29.37]
  Epoch: [006][000/391]   Time 0.217 (0.217)   Data 0.147 (0.147)   Loss 0.6918 (0.6918)   Prec@1 75.781 (75.781)   Prec@5 100.000 (100.000)   [2021-04-17 07:12:46]
  Epoch: [006][200/391]   Time 0.047 (0.048)   Data 0.000 (0.001)   Loss 0.6985 (0.7229)   Prec@1 67.969 (74.658)   Prec@5 99.219 (98.383)   [2021-04-17 07:12:55]
  **Train** Prec@1 74.924 Prec@5 98.458 Error@1 25.076
  **Test** Prec@1 69.460 Prec@5 97.390 Error@1 30.540
  **Test** Prec@1 69.460 Prec@5 97.390 Error@1 30.540

==>>[2021-04-17 07:13:08] [Epoch=007/010] [Need: 00:01:08] [learning_rate=0.1000] [Best : Accuracy=70.63, Error=29.37]
  Epoch: [007][000/391]   Time 0.210 (0.210)   Data 0.164 (0.164)   Loss 0.5985 (0.5985)   Prec@1 84.375 (84.375)   Prec@5 96.875 (96.875)   [2021-04-17 07:13:08]
  Epoch: [007][200/391]   Time 0.047 (0.048)   Data 0.000 (0.001)   Loss 0.5468 (0.7012)   Prec@1 82.031 (75.599)   Prec@5 99.219 (98.542)   [2021-04-17 07:13:18]
  **Train** Prec@1 75.614 Prec@5 98.542 Error@1 24.386
  **Test** Prec@1 74.850 Prec@5 98.440 Error@1 25.150
  **Test** Prec@1 74.850 Prec@5 98.440 Error@1 25.150

==>>[2021-04-17 07:13:31] [Epoch=008/010] [Need: 00:00:45] [learning_rate=0.1000] [Best : Accuracy=74.85, Error=25.15]
  Epoch: [008][000/391]   Time 0.216 (0.216)   Data 0.171 (0.171)   Loss 0.5714 (0.5714)   Prec@1 75.781 (75.781)   Prec@5 100.000 (100.000)   [2021-04-17 07:13:31]
  Epoch: [008][200/391]   Time 0.045 (0.047)   Data 0.000 (0.001)   Loss 0.6104 (0.6790)   Prec@1 82.031 (76.753)   Prec@5 99.219 (98.589)   [2021-04-17 07:13:40]
  **Train** Prec@1 76.496 Prec@5 98.640 Error@1 23.504
  **Test** Prec@1 70.200 Prec@5 97.690 Error@1 29.800
  **Test** Prec@1 70.200 Prec@5 97.690 Error@1 29.800

==>>[2021-04-17 07:13:53] [Epoch=009/010] [Need: 00:00:22] [learning_rate=0.1000] [Best : Accuracy=74.85, Error=25.15]
  Epoch: [009][000/391]   Time 0.215 (0.215)   Data 0.161 (0.161)   Loss 0.6510 (0.6510)   Prec@1 77.344 (77.344)   Prec@5 97.656 (97.656)   [2021-04-17 07:13:53]
  Epoch: [009][200/391]   Time 0.046 (0.049)   Data 0.000 (0.001)   Loss 0.6403 (0.6732)   Prec@1 77.344 (76.683)   Prec@5 97.656 (98.589)   [2021-04-17 07:14:03]
  **Train** Prec@1 76.774 Prec@5 98.584 Error@1 23.226
  **Test** Prec@1 69.880 Prec@5 97.930 Error@1 30.120
  **Test** Prec@1 69.880 Prec@5 97.930 Error@1 30.120
